# Clash Royale Analysis 

> This content is not affiliated with, endorsed, sponsored, or specifically
> approved by Supercell and Supercell is not responsible for it. For more
> information see Supercell’s Fan Content Policy:
> www.supercell.com/fan-content-policy.

An exercise in data collection/analysis, juggling with millions of records.

-------------------------------------------------------------------------------

## Data Collection

First we need data. At the time of writing we use Raspberry Pi 3 Model B
([configuration](https://github.com/S1M0N38/dots/tree/rpi)) to collect data
running a script hourly (`data/collect.sh`). We run another script daily
(`data/join.sh`) to join results from the previous day eliminating redundant
data. Data generated by these two scripts are respectively stored in
`data/db-hour` and in `data/db-day`.

Here's how to setup data collection pipeline on your own machine.
Assuming you have access to command line and [git](https://git-scm.com/)
installed ...

1. Clone this repo in your $HOME directory
```bash 
git clone https://github.com/S1M0N38/cr-analysis.git $HOME/cr-analysis
```

2. Navigate to `data` subdirectory 
```bash
cd $HOME/cr-analysis/data
```
Every **data collection** action must be run from this directory.

3. Ensure `python 3.9` or greater is available.
``` bash 
python -V
```
This project is developed using python 3.9 hence backward compatibility is not
ensured.

4. Upgrade `pip` and install requirements
```bash
python -m pip install --upgrade pip
python -m pip install -r ../requirements.txt
```

5. Create a developer account [here](https://developer.clashroyale.com/#/) and
   add credentials to your env. For example if you use bash add
```bash
export API_CLASH_ROYALE_EMAIL="example@mail.com"
export API_CLASH_ROYALE_PASSWORD="MY_P4S5W0RD"
```
to your `.bashrc`. Then restart the terminal and navigate again to
`cr-analysis/data`.

6. Run test script to ensure that data collection scripts will runs flawlessly.
```bash
./test.sh
```

7. If not error arose you can schedule `collect.sh` to run hourly and `join.sh`
   to run daily.
```bash 
crontab -e
```
This command will open a text file with your favorite editor (choose `nano` if
in doubt). Every line in this file is a scheduled task. Lines that starts with
`#` are just comments. Add those lines at the end of this file.
```crontab
# Clash Royale API credentials
# (you have to add credentials to this file as well)
API_CLASH_ROYALE_EMAIL="example@mail.com"
API_CLASH_ROYALE_PASSWORD="MY_P4S5W0RD"

  0  *  *  *  *  cd $HOME/cr-analysis/data && ./collect.sh
  0 12  *  *  *  cd $HOME/cr-analysis/data && ./join.sh
# |  |  |  |  |  |
# |  |  |  |  |  +--- command
# |  |  |  |  +----- day of week (0 - 6) (Sunday=0)
# |  |  |  +------- month (1 - 12)
# |  |  +--------- day of month (1 - 31)
# |  +----------- hour (0 - 23)
# +------------- min (0 - 59)
```
That's should be all. Scripts scheduled with `crontab` will run in background.
Use a process viewer (e.g. [htop](https://htop.dev/)) to check if the are
running. Alternatively take a look at the `db-hour/collect.log` log file.

### Further Data Manipulation

Let's pretend that tree day pass since you have collection pipeline running on
your machine. You should have a directory structure like this
```
cr-analysis
├── data
│  ├── db-day
│  │  ├── 20221114.csv.gz
│  │  ├── 20221115.csv.gz
│  │  └── 20221116.csv.gz
│  ├── db-hour
│  │  └── ...
│  ├── db-test
│  │  └── ...
│  └── ...
├── README.md
└── requirements.txt
```
In `cr-analysis/data/db-day` are stored compressed CSV files containing battles
collected on that date (This does not mean that battles were played on that day
but that they were played on that day or days before). We copy `db-day`
directory from Raspberry Pi to our local machine with `rsync`:
```bash
rsync -aPv pi@192.168.178.101:cr-analysis/data/db-day .
```
With CSV on our local machine we can proceed with further data manipulation.
For example we like to rearrange battles in such a way that CSV file name
refers to the days battles are played instead of the day battlelogs are
collected. This can be achieved using `split.sh`. This script create `db`
directory like this
```
cr-analysis
├── data
│  ├── db
│  │  ├── 20221109.csv.gz
│  │  ├── 20221110.csv.gz
│  │  ├── 20221111.csv.gz
│  │  ├── 20221112.csv.gz
│  │  ├── 20221113.csv.gz
│  │  ├── 20221114.csv.gz
│  │  ├── 20221115.csv.gz
│  │  └── 20221116.csv.gz
│  ├── db-day
│  │  └── ...
│  ├── db-hour
│  │  └── ...
│  ├── db-test
│  │  └── ...
│  └── ...
├── README.md
└── requirements.txt
```
This means that the file `db/20221111.csv.gz` will contains battle played on
Nov 11 2022 sorted by time. This way of storing data have a couple of
advantages:

- Keep files size relatively small, so they are easy to handle compared to a
  huge CSV file.
- `gzip` compression allow to concatenate them without decompression
  (e.g. `cat 20221109.csv.gz 20221110.csv.gz 20221111.csv.gz`)

For files manipulation `join.sh` and `split.sh` script leverage that power of
command line programs pre-installed on many Unix-Like OS. Take a look at them 
if you want to manipulate compressed CSV on your own.

-------------------------------------------------------------------------------

## Data Analysis

Work in progress ...
